{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a NetCDF file from a time series of MODIS observations.\n",
    "\n",
    "Input datasets (HDF format):\n",
    " - **MxD11_L2** (MODIS [Aqua](https://lpdaac.usgs.gov/products/myd11_l2v006/) and [Terra](https://lpdaac.usgs.gov/products/mod11_l2v006/) *Land Surface Temperature/Emissivity 5-Minute L2 Swath 1 km*)\n",
    " - **MxD03** (MODIS [Aqua](https://modaps.modaps.eosdis.nasa.gov/services/about/products/c6/MYD03.html) and [Terra](https://modaps.modaps.eosdis.nasa.gov/services/about/products/c6/MOD03.html) *Geolocation Fields 5-Min L1A Swath 1km*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import interpolate\n",
    "import cartopy.crs as ccrs\n",
    "import datetime as dt\n",
    "import gdal\n",
    "\n",
    "gdal.UseExceptions()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set plot fonts to a larger size\n",
    "plt.rcParams.update({'font.size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all HDF files in a specified directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfFiles(dirName,ext):\n",
    "    '''Create a list of file names in the given directory with specified file extension.'''\n",
    "    # https://thispointer.com/python-how-to-get-list-of-files-in-directory-and-sub-directories/\n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Only match files with the correct extension\n",
    "        if '.'+ext == os.path.splitext(entry)[1]:\n",
    "            # Create full path and add to list\n",
    "            fullPath = os.path.join(dirName, entry)\n",
    "            allFiles.append(fullPath)       \n",
    "    return allFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 90 LST files, 90 GEO files\n"
     ]
    }
   ],
   "source": [
    "storage = r'//j-lundquist-3.ce.washington.edu/storage'\n",
    "\n",
    "lst_searchDir = storage + r'/MODIS/GrandMesa_MxD11_L2/2017/'\n",
    "lst_file_list = getListOfFiles(lst_searchDir,'hdf')\n",
    "\n",
    "geo_searchDir = storage + r'/MODIS/GrandMesa_MxD03/2017/'\n",
    "geo_file_list = getListOfFiles(geo_searchDir,'hdf')\n",
    "\n",
    "print('Found {} LST files, {} GEO files'.format(len(lst_file_list),len(geo_file_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open files and extract the LST, view angle, latitude, and longitude subdatasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_ds = [gdal.Open('HDF4_EOS:EOS_SWATH:\"{}\":MOD_Swath_LST:LST'.format(path)) for path in lst_file_list]\n",
    "viewangle_ds = [gdal.Open('HDF4_EOS:EOS_SWATH:\"{}\":MOD_Swath_LST:View_angle'.format(path)) for path in lst_file_list]\n",
    "geo_lat_ds = [gdal.Open('HDF4_SDS:UNKNOWN:\"{}\":0'.format(path)) for path in geo_file_list]\n",
    "geo_lon_ds = [gdal.Open('HDF4_SDS:UNKNOWN:\"{}\":1'.format(path)) for path in geo_file_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty arrays to stack data into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = len(lst_ds)\n",
    "m_files = len(geo_lat_ds)\n",
    "# Because the MODIS products are either 2030 or 2040 pixels in 5 minutes...\n",
    "# I'm truncating them all to just 2030\n",
    "along_track_px = 2030\n",
    "cross_track_px = 1354\n",
    "\n",
    "# Create empty arrays to hold the radiance, lat, and lon\n",
    "\n",
    "lst = np.ones((along_track_px,  # Width\n",
    "               cross_track_px), # Height\n",
    "              dtype='float64')\n",
    "\n",
    "viewangle = np.ones((along_track_px,  # Width\n",
    "               cross_track_px), # Height\n",
    "              dtype='float64')\n",
    "\n",
    "lon = np.ones((along_track_px,  # Width\n",
    "               cross_track_px), # Height\n",
    "              dtype='float64')\n",
    "\n",
    "lat = np.ones((along_track_px,  # Width\n",
    "               cross_track_px), # Height\n",
    "              dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match geolocation with LST products, stack into xarray dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files for 2017-02-10 05:45:00.000000\n",
      "Found files for 2017-02-10 17:45:00.000000\n",
      "Found files for 2017-02-11 04:50:00.000000\n",
      "Found files for 2017-02-11 18:30:00.000000\n",
      "Found files for 2017-02-12 05:35:00.000000\n",
      "Found files for 2017-02-12 17:35:00.000000\n",
      "Found files for 2017-02-13 04:40:00.000000\n",
      "Found files for 2017-02-13 18:15:00.000000\n",
      "Found files for 2017-02-14 05:20:00.000000\n",
      "Found files for 2017-02-14 17:20:00.000000\n",
      "Found files for 2017-02-14 19:00:00.000000\n",
      "Found files for 2017-02-15 04:25:00.000000\n",
      "Found files for 2017-02-15 06:05:00.000000\n",
      "Found files for 2017-02-15 18:05:00.000000\n",
      "Found files for 2017-02-16 05:10:00.000000\n",
      "Found files for 2017-02-16 18:50:00.000000\n",
      "Found files for 2017-02-17 05:50:00.000000\n",
      "Found files for 2017-02-17 17:55:00.000000\n",
      "Found files for 2017-02-18 04:55:00.000000\n",
      "Found files for 2017-02-18 18:35:00.000000\n",
      "Found files for 2017-02-19 05:40:00.000000\n",
      "Found files for 2017-02-19 17:40:00.000000\n",
      "Found files for 2017-02-20 04:45:00.000000\n",
      "Found files for 2017-02-20 18:25:00.000000\n",
      "Found files for 2017-02-21 05:30:00.000000\n",
      "Found files for 2017-02-21 17:30:00.000000\n",
      "Found files for 2017-02-22 04:35:00.000000\n",
      "Found files for 2017-02-22 18:10:00.000000\n",
      "Found files for 2017-02-23 05:15:00.000000\n",
      "Found files for 2017-02-23 17:15:00.000000\n",
      "Found files for 2017-02-23 18:55:00.000000\n",
      "Found files for 2017-02-24 04:20:00.000000\n",
      "Found files for 2017-02-24 06:00:00.000000\n",
      "Found files for 2017-02-24 18:00:00.000000\n",
      "Found files for 2017-02-25 05:05:00.000000\n",
      "Found files for 2017-02-25 18:40:00.000000\n",
      "Found files for 2017-02-26 05:45:00.000000\n",
      "Found files for 2017-02-26 17:45:00.000000\n",
      "Found files for 2017-02-27 04:50:00.000000\n",
      "Found files for 2017-02-27 18:30:00.000000\n",
      "Found files for 2017-02-28 05:35:00.000000\n",
      "Found files for 2017-02-28 17:35:00.000000\n",
      "Found files for 2017-03-01 04:40:00.000000\n",
      "Found files for 2017-03-01 18:15:00.000000\n",
      "Found files for 2017-02-10 08:20:00.000000\n",
      "Found files for 2017-02-10 10:00:00.000000\n",
      "Found files for 2017-02-10 19:25:00.000000\n",
      "Found files for 2017-02-10 21:05:00.000000\n",
      "Found files for 2017-02-11 09:05:00.000000\n",
      "Found files for 2017-02-11 20:10:00.000000\n",
      "Found files for 2017-02-12 09:45:00.000000\n",
      "Found files for 2017-02-12 20:50:00.000000\n",
      "Found files for 2017-02-13 08:50:00.000000\n",
      "Found files for 2017-02-13 19:55:00.000000\n",
      "Found files for 2017-02-14 09:35:00.000000\n",
      "Found files for 2017-02-14 20:40:00.000000\n",
      "Found files for 2017-02-15 08:40:00.000000\n",
      "Found files for 2017-02-15 19:45:00.000000\n",
      "Found files for 2017-02-16 09:25:00.000000\n",
      "Found files for 2017-02-16 20:25:00.000000\n",
      "Found files for 2017-02-17 08:30:00.000000\n",
      "Found files for 2017-02-17 10:05:00.000000\n",
      "Found files for 2017-02-17 19:30:00.000000\n",
      "Found files for 2017-02-17 21:10:00.000000\n",
      "Found files for 2017-02-18 09:10:00.000000\n",
      "Found files for 2017-02-18 20:15:00.000000\n",
      "Found files for 2017-02-19 09:55:00.000000\n",
      "Found files for 2017-02-19 21:00:00.000000\n",
      "Found files for 2017-02-20 09:00:00.000000\n",
      "Found files for 2017-02-20 20:05:00.000000\n",
      "Found files for 2017-02-21 09:40:00.000000\n",
      "Found files for 2017-02-21 20:45:00.000000\n",
      "Found files for 2017-02-22 08:45:00.000000\n",
      "Found files for 2017-02-22 19:50:00.000000\n",
      "Found files for 2017-02-23 09:30:00.000000\n",
      "Found files for 2017-02-23 20:35:00.000000\n",
      "Found files for 2017-02-24 08:35:00.000000\n",
      "Found files for 2017-02-24 19:40:00.000000\n",
      "Found files for 2017-02-25 09:15:00.000000\n",
      "Found files for 2017-02-25 20:20:00.000000\n",
      "Found files for 2017-02-26 08:20:00.000000\n",
      "Found files for 2017-02-26 10:00:00.000000\n",
      "Found files for 2017-02-26 19:25:00.000000\n",
      "Found files for 2017-02-26 21:05:00.000000\n",
      "Found files for 2017-02-27 09:05:00.000000\n",
      "Found files for 2017-02-27 20:10:00.000000\n",
      "Found files for 2017-02-28 09:45:00.000000\n",
      "Found files for 2017-02-28 20:50:00.000000\n",
      "Found files for 2017-03-01 08:50:00.000000\n",
      "Found files for 2017-03-01 19:55:00.000000\n",
      "90 out of 90 files loaded with geolocation information\n"
     ]
    }
   ],
   "source": [
    "# LST scale factor\n",
    "lst_scale_factor=0.02\n",
    "# View angle scale factor\n",
    "view_scale_factor = 0.5\n",
    "# For each file in the timeseries of MODIS observations\n",
    "k = 0\n",
    "for i in range(0,n_files):\n",
    "    # Read the date and time from the LST product\n",
    "    date = lst_ds[i].GetMetadataItem('RANGEBEGINNINGDATE')\n",
    "    time = lst_ds[i].GetMetadataItem('RANGEBEGINNINGTIME')\n",
    "    eq_cross_time = lst_ds[i].GetMetadataItem('EQUATORCROSSINGTIME.1')\n",
    "    # Find its matching MxD03 Geolocation product\n",
    "    for j in range(0,m_files):\n",
    "        # date_geo = geo_lat_ds[j].GetMetadataItem('RANGEBEGINNINGDATE')\n",
    "        # time_geo = geo_lat_ds[j].GetMetadataItem('RANGEBEGINNINGTIME')\n",
    "        eq_cross_time_geo = geo_lat_ds[j].GetMetadataItem('EQUATORCROSSINGTIME.1')\n",
    "        # Once we find a match, load these into our arrays\n",
    "        if eq_cross_time==eq_cross_time_geo:# and lst_ds[i].ReadAsArray() is not None:\n",
    "            print('Found files for {} {}'.format(date,time))\n",
    "            # Load the LST values and scale them\n",
    "            lst = lst_scale_factor * lst_ds[i].ReadAsArray()[0:along_track_px,0:cross_track_px]\n",
    "            # Replace the nodata value 0, with Nans\n",
    "            lst[lst==0.] = np.nan\n",
    "            # Read the view angles from the LST product, scale and remove nodata values\n",
    "            viewangle = viewangle_ds[i].ReadAsArray()[0:along_track_px,0:cross_track_px].astype(float)\n",
    "            viewangle[viewangle==255] = np.nan\n",
    "            viewangle = view_scale_factor * viewangle\n",
    "            # Read the latitudes and longitudes from the geolocaiton product\n",
    "            lat = geo_lat_ds[j].ReadAsArray()[0:along_track_px,0:cross_track_px]\n",
    "            lon = geo_lon_ds[j].ReadAsArray()[0:along_track_px,0:cross_track_px]\n",
    "            # Add the MODIS data to an xarray dataset\n",
    "            if k==0: # If this is our first file, make a new dataset\n",
    "                ds = xr.Dataset({'lst': (['line', 'pixel'],  lst),\n",
    "                                'viewangle': (['line', 'pixel'],  viewangle)},\n",
    "                                coords={'longitude': (['line', 'pixel'], lon),\n",
    "                                        'latitude': (['line', 'pixel'], lat),\n",
    "                                        'time': pd.to_datetime('{} {}'.format(date,time))})\n",
    "                k+=1 # add to the successful file load counter\n",
    "            else: # append to existing dataset\n",
    "                ds_new = xr.Dataset({'lst': (['line', 'pixel'],  lst),\n",
    "                                'viewangle': (['line', 'pixel'],  viewangle)},\n",
    "                                coords={'longitude': (['line', 'pixel'], lon),\n",
    "                                        'latitude': (['line', 'pixel'], lat),\n",
    "                                        'time': pd.to_datetime('{} {}'.format(date,time))})\n",
    "                ds = xr.concat([ds,ds_new],'time') # concatenate along the time axis\n",
    "                k+=1 # add to the successful file load counter\n",
    "            break # go to next LST file\n",
    "\n",
    "# Finally sort by time:\n",
    "ds = ds.sortby(ds.time)\n",
    "print('{} out of {} files loaded with geolocation information'.format(k,n_files))\n",
    "\n",
    "# Update to the number of files we successfully loaded\n",
    "n_files = ds.time.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export this stack of MODIS observations as a new NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (line: 2030, pixel: 1354, time: 90)\n",
       "Coordinates:\n",
       "    longitude  (time, line, pixel) float32 -101.01039 -101.05827 ... -116.95163\n",
       "    latitude   (time, line, pixel) float32 26.13496 26.132174 ... 39.706432\n",
       "  * time       (time) datetime64[ns] 2017-02-10T05:45:00 ... 2017-03-01T19:55:00\n",
       "Dimensions without coordinates: line, pixel\n",
       "Data variables:\n",
       "    lst        (time, line, pixel) float64 284.9 283.4 283.2 ... nan 272.7 nan\n",
       "    viewangle  (time, line, pixel) float64 65.5 65.5 65.0 65.0 ... nan 65.0 nan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf('./nc/grandmesa-201702100545-201703011955-MxD11_L2-MxD03.nc',mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "work in progress...\n",
    "\n",
    "Subset to some bounding box around our site of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the coordinates of a bounding box around our site of interest\n",
    "\n",
    "lat_obs = 37.88 \n",
    "long_obs = -119.31\n",
    "\n",
    "n_bound = lat_obs + 1\n",
    "s_bound = lat_obs - 1\n",
    "e_bound = long_obs + 1\n",
    "w_bound = long_obs - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the datasets to only within this bounding box\n",
    "\n",
    "ds2 = ds.where( (ds.latitude>=s_bound) & \n",
    "               (ds.latitude<=n_bound) & \n",
    "               (ds.longitude>=w_bound) & \n",
    "               (ds.longitude<=e_bound), \n",
    "               drop=True) # this should drop coordinate elements that are completely nan (smaller line/pixel dims?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot all the resulting images from this bounding box\n",
    "#\n",
    "#for i in range(ds2.time.size):\n",
    "#    ds2.lst[i].plot(x='longitude',y='latitude',add_colorbar=False)\n",
    "#    plt.xlim((w_bound,e_bound))\n",
    "#    plt.ylim((s_bound,n_bound))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export this stack of MODIS observations as a new NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (line: 2030, pixel: 1354, time: 83)\n",
       "Coordinates:\n",
       "    longitude  (time, line, pixel) float32 -127.85866 -127.798355 ... -150.81458\n",
       "    latitude   (time, line, pixel) float32 55.418823 55.41911 ... 44.437504\n",
       "  * time       (time) datetime64[ns] 2017-04-10T09:40:00 ... 2017-04-30T22:00:00\n",
       "Dimensions without coordinates: line, pixel\n",
       "Data variables:\n",
       "    lst        (time, line, pixel) float64 nan nan nan nan ... nan nan nan nan\n",
       "    viewangle  (time, line, pixel) float64 nan nan nan nan ... nan nan nan nan"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.to_netcdf('./nc/tuolumne_bbox-201704151925-201704302200-MxD11_L2-MxD03.nc',mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
